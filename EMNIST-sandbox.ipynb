{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "import numpy as np\n",
    "\n",
    "# Read all EMNIST test and train data\n",
    "mndata = MNIST('data')\n",
    "X_train, y_train = mndata.load('data/emnist-byclass-train-images-idx3-ubyte', \n",
    "                               'data/emnist-byclass-train-labels-idx1-ubyte')\n",
    "X_test, y_test = mndata.load('data/emnist-byclass-test-images-idx3-ubyte', \n",
    "                             'data/emnist-byclass-test-labels-idx1-ubyte')\n",
    "# Read mapping of the labels and convert ASCII values to chars\n",
    "mapping = []\n",
    "with open('data/emnist-byclass-mapping.txt') as f:\n",
    "    for line in f:\n",
    "        mapping.append(chr(int(line.split()[1])))\n",
    "\n",
    "# Convert data to numpy arrays and normalize images to the interval [0, 1]\n",
    "X_train = np.array(X_train) / 255\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test) / 255\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's start with Machine Learning!\n",
    "### Random Forests are cool, let's use them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.788464974142\n",
      "      Std dev:  0.00166719277345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
    "cv_scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "print('Mean accuracy: ', cv_scores.mean())\n",
    "print('      Std dev: ', cv_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we have enough memory, we can use scikit-learn's GridSearchCV to optimize the RandomForestClassifier parameters (n_estimators, in this case). Since this will use A LOT of memory (and likely raise a MemoryError), we can try a few more examples by hand.\n",
    "\n",
    "This is done below, with 25 and 50 estimators respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=25, n_jobs=-1)\n",
    "cv_scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "print('Mean accuracy: ', cv_scores.mean())\n",
    "print('      Std dev: ', cv_scores.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy:  0.822484214604\n",
      "      Std dev:  0.00147729065302\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "cv_scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "print('Mean accuracy: ', cv_scores.mean())\n",
    "print('      Std dev: ', cv_scores.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Random Forest with 50 estimators seems to do a good job, so let's confirm it by evaluating the accuracy of this model when using the EMNIST test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test set:  0.825804011245\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy on test set: ', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do some Deep Learning!\n",
    "### Convolution Neural Networks are the way to go here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's set up our CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "\n",
    "def build_model(nb_classes, nb_filters, kernel_size, pool_size, input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(int(nb_filters / 2), kernel_size, padding='valid',\n",
    "                            input_shape=input_shape, activation='relu',\n",
    "                            kernel_initializer='he_normal', data_format = 'channels_first'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    model.add(Convolution2D(nb_filters, kernel_size, activation='relu', \n",
    "                            kernel_initializer='he_normal', data_format = 'channels_first'))\n",
    "    model.add(MaxPooling2D(pool_size=pool_size))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(250, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(125, activation='relu', kernel_initializer='he_normal'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(nb_classes, activation='softmax', kernel_initializer='he_normal'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we should set the parameters to be used with our CNN and preprocess the data a bit further in order for it to be in the necessary shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "# Number of classes in the train set\n",
    "nb_classes = len(mapping)\n",
    "# Number of convolutional filters\n",
    "nb_filters = 32\n",
    "# Convolutional kernel size\n",
    "kernel_size = (5, 5) # convolution kernel size\n",
    "# Size of pooling area\n",
    "pool_size = (2, 2)\n",
    "# Shape of the images (color channels, width, height)\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# Reshape data to be used in a Convolutional Neural Network\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
    "\n",
    "# One-hot encoding of the label arrays\n",
    "y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we can finally build our CNN model and fit it with the EMNIST training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 16, 24, 24)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 8, 12, 24)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 8, 20)         6432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 4, 20)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 250)               320250    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 125)               31375     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 125)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 62)                7812      \n",
      "=================================================================\n",
      "Total params: 366,285\n",
      "Trainable params: 366,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = build_model(nb_classes, nb_filters, kernel_size, pool_size, input_shape)\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trained, we can evaluate our CNN with the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116256/116323 [============================>.] - ETA: 0s\n",
      "    Test set loss: 0.431493847698\n",
      "Test set accuracy: 0.846651135202\n"
     ]
    }
   ],
   "source": [
    "test_eval = model.evaluate(X_test, y_test)\n",
    "print()\n",
    "print('    Test set loss:', test_eval[0])\n",
    "print('Test set accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
